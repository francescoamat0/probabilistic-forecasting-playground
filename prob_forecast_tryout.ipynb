{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75058896",
   "metadata": {},
   "source": [
    "# Modern Probabilistic Forecasting Methods\n",
    "\n",
    "This notebook demonstrates end-to-end examples of important probabilistic forecasting methods using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af35d30",
   "metadata": {},
   "source": [
    "## 0) Common setup: data + utilities\n",
    "\n",
    "All examples use the same synthetic dataset: one daily series with covariates (trend + seasonality + promo + price).\n",
    "\n",
    "The DGP is deliberately realistic and **hard**: heteroscedastic noise, heavy tails (Student-t), occasional outliers, a structural break, holiday effects, and covariate interactions. This ensures different methods show genuine strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322183e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "# --- Realistic synthetic daily data ---\n",
    "# This DGP is deliberately \"hard\" so different methods show real strengths/weaknesses:\n",
    "#   - heteroscedastic noise  → constant-variance models (SARIMAX) will miscalibrate\n",
    "#   - heavy tails (Student-t) → Gaussian assumptions underestimate tail risk\n",
    "#   - occasional outliers     → tests robustness\n",
    "#   - structural break        → favours models that weight recent data\n",
    "#   - covariate interactions  → tree-based / deep models should outperform linear\n",
    "\n",
    "n = 900\n",
    "ds = pd.date_range(\"2022-01-01\", periods=n, freq=\"D\")\n",
    "t = np.arange(n)\n",
    "\n",
    "# ── Covariates ──\n",
    "promo = (rng.random(n) < 0.12).astype(int)\n",
    "price = 10 + 0.02*t + rng.normal(0, 0.4, n) - 0.8*promo\n",
    "\n",
    "# ── Deterministic components ──\n",
    "weekly  = 1.2 * np.sin(2*np.pi*t/7)\n",
    "yearly  = 2.0 * np.sin(2*np.pi*t/365.25)\n",
    "trend   = 0.01 * t\n",
    "\n",
    "# ── Structural break: level shift at ~day 500 (mid-2023) ──\n",
    "level_shift = 2.5 * (t >= 500).astype(float)\n",
    "\n",
    "# ── Holiday-like effects (Christmas/NY and Easter-ish windows) ──\n",
    "holidays = np.zeros(n)\n",
    "for i, d in enumerate(ds):\n",
    "    if (d.month == 12 and d.day >= 23) or (d.month == 1 and d.day <= 2):\n",
    "        holidays[i] = rng.uniform(3, 6)       # strong Christmas / New Year lift\n",
    "    elif d.month == 4 and 10 <= d.day <= 17:\n",
    "        holidays[i] = rng.uniform(1, 3)        # Easter-ish bump\n",
    "\n",
    "# ── Covariate effects with interaction ──\n",
    "promo_effect = 3.0*promo + 0.5*promo*(price - price.mean())   # promo x price interaction\n",
    "price_effect = -0.6 * price\n",
    "\n",
    "# ── Heteroscedastic noise: scale varies with local level ──\n",
    "base_level  = 20 + trend + yearly\n",
    "noise_scale = 0.8 + 0.06 * np.abs(base_level - base_level.mean())   # ranges ~0.8 – 2.0\n",
    "noise = rng.standard_t(df=5, size=n) * noise_scale                  # heavy tails (df=5)\n",
    "\n",
    "# ── Outliers: ~1.5% of days get a large shock ──\n",
    "outlier_mask = rng.random(n) < 0.015\n",
    "outliers = outlier_mask * rng.choice([-1, 1], size=n) * rng.uniform(4, 8, size=n)\n",
    "\n",
    "# ── Assemble target ──\n",
    "y = (20 + trend + weekly + yearly + level_shift\n",
    "     + holidays + promo_effect + price_effect\n",
    "     + noise + outliers)\n",
    "\n",
    "df = pd.DataFrame({\"ds\": ds, \"y\": y, \"promo\": promo, \"price\": price})\n",
    "df = df.set_index(\"ds\")\n",
    "\n",
    "print(f\"Series length: {len(df)}, range: [{df['y'].min():.1f}, {df['y'].max():.1f}]\")\n",
    "print(f\"Outlier days: {outlier_mask.sum()}, structural break at day 500\")\n",
    "\n",
    "# --- train / test split ---\n",
    "h = 60  # forecast horizon\n",
    "train = df.iloc[:-h].copy()\n",
    "test  = df.iloc[-h:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed403545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lag_features(data: pd.DataFrame, lags=(1,2,7,14,28), rolls=(7,14,28)) -> pd.DataFrame:\n",
    "    out = data.copy()\n",
    "    for L in lags:\n",
    "        out[f\"lag_{L}\"] = out[\"y\"].shift(L)\n",
    "    for R in rolls:\n",
    "        out[f\"roll_mean_{R}\"] = out[\"y\"].shift(1).rolling(R).mean()\n",
    "        out[f\"roll_std_{R}\"] = out[\"y\"].shift(1).rolling(R).std()\n",
    "    # calendar features (often helps GBDT)\n",
    "    idx = out.index\n",
    "    out[\"dow\"] = idx.dayofweek\n",
    "    out[\"month\"] = idx.month\n",
    "    return out\n",
    "\n",
    "def pinball_loss(y_true, y_pred_q, q: float) -> float:\n",
    "    # y_pred_q is quantile prediction at q\n",
    "    e = y_true - y_pred_q\n",
    "    return float(np.mean(np.maximum(q*e, (q-1)*e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed25267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_forecast_with_interval(\n",
    "    idx, y_true, p50, p10=None, p90=None, title=\"Forecast with interval\"\n",
    "):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(idx, y_true, label=\"y_true\")\n",
    "    plt.plot(idx, p50, label=\"p50 / mean\")\n",
    "    if p10 is not None and p90 is not None:\n",
    "        plt.fill_between(idx, p10, p90, alpha=0.2, label=\"P10–P90\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def empirical_coverage(y_true, lo, hi) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    lo = np.asarray(lo)\n",
    "    hi = np.asarray(hi)\n",
    "    return float(np.mean((y_true >= lo) & (y_true <= hi)))\n",
    "\n",
    "def mean_interval_width(lo, hi) -> float:\n",
    "    return float(np.mean(np.asarray(hi) - np.asarray(lo)))\n",
    "\n",
    "def wis_approx_10_50_90(y_true, p10, p50, p90) -> float:\n",
    "    \"\"\"\n",
    "    Approximate Weighted Interval Score using only {0.1,0.5,0.9}.\n",
    "    This is not the full multi-alpha WIS, but is a useful single-number proxy.\n",
    "\n",
    "    For central 80% interval: alpha=0.2 (since 10-90)\n",
    "    Interval score = (U-L) + (2/alpha)*(L-y)*1(y<L) + (2/alpha)*(y-U)*1(y>U)\n",
    "    \"\"\"\n",
    "    y = np.asarray(y_true)\n",
    "    L = np.asarray(p10)\n",
    "    U = np.asarray(p90)\n",
    "    alpha = 0.2\n",
    "    width = U - L\n",
    "    under = (L - y) * (y < L)\n",
    "    over  = (y - U) * (y > U)\n",
    "    score_interval = width + (2/alpha) * (under + over)\n",
    "\n",
    "    # Add absolute error of median (often included in WIS variants)\n",
    "    score_median = np.abs(y - np.asarray(p50))\n",
    "\n",
    "    return float(np.mean(score_median + 0.5 * score_interval))\n",
    "\n",
    "def evaluate_probabilistic_forecast(y_true, p10, p50, p90, label=\"model\"):\n",
    "    y_true = np.asarray(y_true)\n",
    "    p10 = np.asarray(p10); p50 = np.asarray(p50); p90 = np.asarray(p90)\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": label,\n",
    "        \"pinball_0.1\": pinball_loss(y_true, p10, 0.1),\n",
    "        \"pinball_0.5\": pinball_loss(y_true, p50, 0.5),\n",
    "        \"pinball_0.9\": pinball_loss(y_true, p90, 0.9),\n",
    "        \"coverage_80(P10-P90)\": empirical_coverage(y_true, p10, p90),\n",
    "        \"mean_width(P10-P90)\": mean_interval_width(p10, p90),\n",
    "        \"WIS_proxy(10/50/90)\": wis_approx_10_50_90(y_true, p10, p50, p90),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics: dict):\n",
    "    # pretty print\n",
    "    keys = [k for k in metrics.keys() if k != \"model\"]\n",
    "    print(f\"\\n== {metrics['model']} ==\")\n",
    "    for k in keys:\n",
    "        v = metrics[k]\n",
    "        if \"coverage\" in k:\n",
    "            print(f\"{k:>22}: {v:.3f}\")\n",
    "        else:\n",
    "            print(f\"{k:>22}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_backtest_indices(n_total, horizon, n_folds=5, min_train=300):\n",
    "    \"\"\"\n",
    "    Returns folds of (train_end, test_start, test_end) indices for rolling-origin evaluation.\n",
    "    \"\"\"\n",
    "    assert min_train + horizon < n_total\n",
    "    last_start = n_total - horizon\n",
    "    fold_starts = np.linspace(min_train, last_start, n_folds, dtype=int)\n",
    "    folds = []\n",
    "    for train_end in fold_starts:\n",
    "        test_start = train_end\n",
    "        test_end = train_end + horizon\n",
    "        if test_end <= n_total:\n",
    "            folds.append((train_end, test_start, test_end))\n",
    "    return folds\n",
    "\n",
    "def aggregate_backtest_metrics(metrics_list):\n",
    "    dfm = pd.DataFrame(metrics_list).drop(columns=[\"model\"], errors=\"ignore\")\n",
    "    return dfm.mean().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce0dc8",
   "metadata": {},
   "source": [
    "## 1) Classical probabilistic: SARIMAX (ARIMAX) in statsmodels\n",
    "\n",
    "This is a solid baseline when you want interpretability and a well-defined stochastic model. It supports exogenous covariates and yields prediction intervals via the state-space machinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Exogenous regressors (must be aligned)\n",
    "exog_cols = [\"promo\", \"price\"]\n",
    "\n",
    "# A reasonable starting SARIMAX spec (you'd tune this)\n",
    "model = sm.tsa.SARIMAX(\n",
    "    train[\"y\"],\n",
    "    exog=train[exog_cols],\n",
    "    order=(1,1,1),\n",
    "    seasonal_order=(1,0,1,7),\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False,\n",
    ")\n",
    "\n",
    "res = model.fit(disp=False)\n",
    "\n",
    "# Forecast with exog for future horizon\n",
    "fc = res.get_forecast(steps=h, exog=test[exog_cols])\n",
    "mean = fc.predicted_mean\n",
    "ci = fc.conf_int(alpha=0.10)  # 90% interval\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"y_true\": test[\"y\"],\n",
    "    \"y_mean\": mean,\n",
    "    \"y_lo_90\": ci.iloc[:, 0],\n",
    "    \"y_hi_90\": ci.iloc[:, 1],\n",
    "}, index=test.index)\n",
    "\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661cac6",
   "metadata": {},
   "source": [
    "### Notes you'd care about in production\n",
    "\n",
    "- SARIMAX intervals can be miscalibrated if the noise model is wrong or residuals aren't close to assumptions; calibration checks are mandatory.\n",
    "- Great when seasonality is stable and the business wants \"a model you can explain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert SARIMAX CI to p10/p50/p90 (approx) ---\n",
    "# NOTE: statsmodels gives symmetric interval based on Gaussian assumptions;\n",
    "# conf_int(alpha=0.10) corresponds to central 90% (P05-P95). We asked alpha=0.10 earlier,\n",
    "# so those bounds are 5% and 95% not 10/90. We'll recompute alpha=0.20 for 10/90.\n",
    "ci_80 = res.get_forecast(steps=h, exog=test[exog_cols]).conf_int(alpha=0.20)\n",
    "\n",
    "p50 = mean.values\n",
    "p10 = ci_80.iloc[:, 0].values\n",
    "p90 = ci_80.iloc[:, 1].values\n",
    "\n",
    "# --- Plot ---\n",
    "plot_forecast_with_interval(\n",
    "    idx=test.index,\n",
    "    y_true=test[\"y\"].values,\n",
    "    p50=p50, p10=p10, p90=p90,\n",
    "    title=\"SARIMAX forecast (P10–P90 from state-space CI)\"\n",
    ")\n",
    "\n",
    "# --- Evaluate on the holdout horizon ---\n",
    "m_sarimax = evaluate_probabilistic_forecast(test[\"y\"].values, p10, p50, p90, label=\"SARIMAX\")\n",
    "print_metrics(m_sarimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875de962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rolling-origin backtest (optional) ---\n",
    "metrics_bt = []\n",
    "folds = rolling_backtest_indices(n_total=len(df), horizon=h, n_folds=4, min_train=400)\n",
    "\n",
    "for train_end, test_start, test_end in folds:\n",
    "    tr = df.iloc[:train_end]\n",
    "    te = df.iloc[test_start:test_end]\n",
    "\n",
    "    mod = sm.tsa.SARIMAX(\n",
    "        tr[\"y\"], exog=tr[exog_cols],\n",
    "        order=(1,1,1), seasonal_order=(1,0,1,7),\n",
    "        enforce_stationarity=False, enforce_invertibility=False,\n",
    "    )\n",
    "    r = mod.fit(disp=False)\n",
    "    fc = r.get_forecast(steps=h, exog=te[exog_cols])\n",
    "    p50 = fc.predicted_mean.values\n",
    "    ci_80 = fc.conf_int(alpha=0.20)\n",
    "    p10 = ci_80.iloc[:, 0].values\n",
    "    p90 = ci_80.iloc[:, 1].values\n",
    "\n",
    "    metrics_bt.append(evaluate_probabilistic_forecast(te[\"y\"].values, p10, p50, p90, label=\"SARIMAX\"))\n",
    "\n",
    "bt_avg = aggregate_backtest_metrics(metrics_bt)\n",
    "print(\"\\nSARIMAX backtest avg metrics:\", bt_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d4875",
   "metadata": {},
   "source": [
    "## 2) Industry workhorse: LightGBM quantile regression (multiple quantiles)\n",
    "\n",
    "Train one model per quantile (simple, very common), using lag/rolling features + covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166cbbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "feat_df = make_lag_features(df)\n",
    "feat_df = feat_df.dropna()\n",
    "\n",
    "train_f = feat_df.iloc[:-h]\n",
    "test_f  = feat_df.iloc[-h:]\n",
    "\n",
    "features = [c for c in train_f.columns if c != \"y\"]\n",
    "\n",
    "X_train, y_train = train_f[features], train_f[\"y\"]\n",
    "X_test, y_test   = test_f[features], test_f[\"y\"]\n",
    "\n",
    "def fit_lgb_quantile(q: float):\n",
    "    params = dict(\n",
    "        objective=\"quantile\",\n",
    "        alpha=q,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=63,\n",
    "        min_data_in_leaf=50,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.9,\n",
    "        bagging_freq=1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    booster = lgb.train(params, dtrain, num_boost_round=2000, valid_sets=[dtrain], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    return booster\n",
    "\n",
    "qs = [0.1, 0.5, 0.9]\n",
    "models = {q: fit_lgb_quantile(q) for q in qs}\n",
    "\n",
    "preds = {q: models[q].predict(X_test) for q in qs}\n",
    "\n",
    "lgb_out = pd.DataFrame({\n",
    "    \"y_true\": y_test.values,\n",
    "    \"p10\": preds[0.1],\n",
    "    \"p50\": preds[0.5],\n",
    "    \"p90\": preds[0.9],\n",
    "}, index=test_f.index)\n",
    "\n",
    "# quick metrics\n",
    "print(\"Pinball(0.1):\", pinball_loss(y_test.values, lgb_out[\"p10\"].values, 0.1))\n",
    "print(\"Pinball(0.5):\", pinball_loss(y_test.values, lgb_out[\"p50\"].values, 0.5))\n",
    "print(\"Pinball(0.9):\", pinball_loss(y_test.values, lgb_out[\"p90\"].values, 0.9))\n",
    "\n",
    "lgb_out.head()\n",
    "\n",
    "# ── Plot + Evaluate ──\n",
    "p10 = lgb_out[\"p10\"].values\n",
    "p50 = lgb_out[\"p50\"].values\n",
    "p90 = lgb_out[\"p90\"].values\n",
    "\n",
    "plot_forecast_with_interval(\n",
    "    idx=lgb_out.index,\n",
    "    y_true=lgb_out[\"y_true\"].values,\n",
    "    p50=p50, p10=p10, p90=p90,\n",
    "    title=\"LightGBM quantile forecast (P10-P90)\"\n",
    ")\n",
    "\n",
    "m_lgbm = evaluate_probabilistic_forecast(lgb_out[\"y_true\"].values, p10, p50, p90, label=\"LightGBM-Quantiles\")\n",
    "print_metrics(m_lgbm)\n",
    "\n",
    "# ── Rolling-origin backtest ──\n",
    "metrics_bt = []\n",
    "folds = rolling_backtest_indices(n_total=len(feat_df), horizon=h, n_folds=5, min_train=400)\n",
    "\n",
    "for train_end, test_start, test_end in folds:\n",
    "    tr = feat_df.iloc[:train_end].dropna()\n",
    "    te = feat_df.iloc[test_start:test_end].dropna()\n",
    "\n",
    "    Xtr, ytr = tr[features], tr[\"y\"]\n",
    "    Xte, yte = te[features], te[\"y\"]\n",
    "\n",
    "    def fit_q(q):\n",
    "        params = dict(objective=\"quantile\", alpha=q, learning_rate=0.05, num_leaves=63,\n",
    "                      min_data_in_leaf=50, feature_fraction=0.9, bagging_fraction=0.9,\n",
    "                      bagging_freq=1, verbose=-1)\n",
    "        dtr = lgb.Dataset(Xtr, label=ytr)\n",
    "        return lgb.train(params, dtr, num_boost_round=1500, valid_sets=[dtr],\n",
    "                         callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "\n",
    "    m10, m50, m90 = fit_q(0.1), fit_q(0.5), fit_q(0.9)\n",
    "    p10 = m10.predict(Xte); p50 = m50.predict(Xte); p90 = m90.predict(Xte)\n",
    "\n",
    "    metrics_bt.append(evaluate_probabilistic_forecast(yte.values, p10, p50, p90, label=\"LGBM\"))\n",
    "\n",
    "bt_avg = aggregate_backtest_metrics(metrics_bt)\n",
    "print(\"\\nLightGBM backtest avg metrics:\", bt_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121c62c",
   "metadata": {},
   "source": [
    "### Production notes\n",
    "\n",
    "- This is usually paired with: strong backtesting, covariate availability guarantees, leakage checks, and monitoring for feature drift.\n",
    "- Quantiles can cross (P90 < P50) unless you enforce monotonicity post-hoc (e.g., isotonic fix) or use joint quantile approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44892201",
   "metadata": {},
   "source": [
    "## 3) \"Distributional boosting\": NGBoost (predict a full distribution)\n",
    "\n",
    "Instead of quantiles, you fit parameters of a distribution (e.g., Normal) via natural-gradient boosting; very practical when you want a proper probabilistic model with likelihood-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ngboost\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.distns import Normal\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Use same feature matrix as LightGBM example\n",
    "base = HistGradientBoostingRegressor(max_depth=6, learning_rate=0.05)\n",
    "ngb = NGBRegressor(Dist=Normal, Base=base, n_estimators=800, verbose=False, random_state=7)\n",
    "\n",
    "ngb.fit(X_train, y_train)\n",
    "\n",
    "dist = ngb.pred_dist(X_test)  # distribution object\n",
    "p10 = dist.ppf(0.1)\n",
    "p50 = dist.ppf(0.5)\n",
    "p90 = dist.ppf(0.9)\n",
    "\n",
    "ngb_out = pd.DataFrame({\"y_true\": y_test, \"p10\": p10, \"p50\": p50, \"p90\": p90}, index=test_f.index)\n",
    "ngb_out.head()\n",
    "\n",
    "# ── Plot + Evaluate ──\n",
    "p10 = ngb_out[\"p10\"].values\n",
    "p50 = ngb_out[\"p50\"].values\n",
    "p90 = ngb_out[\"p90\"].values\n",
    "\n",
    "plot_forecast_with_interval(\n",
    "    idx=ngb_out.index,\n",
    "    y_true=ngb_out[\"y_true\"].values,\n",
    "    p50=p50, p10=p10, p90=p90,\n",
    "    title=\"NGBoost (Normal) forecast (P10-P90)\"\n",
    ")\n",
    "\n",
    "m_ngboost = evaluate_probabilistic_forecast(ngb_out[\"y_true\"].values, p10, p50, p90, label=\"NGBoost-Normal\")\n",
    "print_metrics(m_ngboost)\n",
    "\n",
    "# ── Rolling-origin backtest ──\n",
    "metrics_bt = []\n",
    "folds = rolling_backtest_indices(n_total=len(feat_df), horizon=h, n_folds=5, min_train=400)\n",
    "\n",
    "for train_end, test_start, test_end in folds:\n",
    "    tr = feat_df.iloc[:train_end].dropna()\n",
    "    te = feat_df.iloc[test_start:test_end].dropna()\n",
    "\n",
    "    Xtr, ytr = tr[features], tr[\"y\"]\n",
    "    Xte, yte = te[features], te[\"y\"]\n",
    "\n",
    "    base = HistGradientBoostingRegressor(max_depth=6, learning_rate=0.05)\n",
    "    ngb = NGBRegressor(Dist=Normal, Base=base, n_estimators=600, verbose=False, random_state=7)\n",
    "    ngb.fit(Xtr, ytr)\n",
    "\n",
    "    dist = ngb.pred_dist(Xte)\n",
    "    p10 = dist.ppf(0.1); p50 = dist.ppf(0.5); p90 = dist.ppf(0.9)\n",
    "\n",
    "    metrics_bt.append(evaluate_probabilistic_forecast(yte.values, p10, p50, p90, label=\"NGBoost\"))\n",
    "\n",
    "bt_avg = aggregate_backtest_metrics(metrics_bt)\n",
    "print(\"\\nNGBoost backtest avg metrics:\", bt_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0086094",
   "metadata": {},
   "source": [
    "### Production notes\n",
    "\n",
    "- You get a coherent distribution (no quantile crossing).\n",
    "- But your distributional assumption (Normal, Student-t, etc.) matters; mis-specification can hurt calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158cc35",
   "metadata": {},
   "source": [
    "## 4) Calibration layer: Conformal prediction intervals (MAPIE) on top of any model\n",
    "\n",
    "Conformal is a huge deal operationally because it gives distribution-free coverage guarantees (under conditions) and works with any regressor. MAPIE includes time-series examples.\n",
    "\n",
    "Here's a practical pattern: fit a point model, then conformalize to get calibrated intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mapie scikit-learn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from mapie.regression import TimeSeriesRegressor  # renamed from MapieTimeSeriesRegressor in MAPIE >=1.0\n",
    "\n",
    "base_model = HistGradientBoostingRegressor(max_depth=6, learning_rate=0.05, random_state=7)\n",
    "\n",
    "# MAPIE time-series conformal with rolling splits\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mapie = TimeSeriesRegressor(\n",
    "    estimator=base_model,\n",
    "    cv=cv,\n",
    "    method=\"enbpi\",     # EnbPI-style for time series\n",
    "    agg_function=\"mean\" # aggregation of ensemble predictions\n",
    ")\n",
    "\n",
    "mapie.fit(X_train, y_train)\n",
    "\n",
    "# NOTE: In MAPIE >=1.0, predict() uses `confidence_level=` instead of `alpha=`.\n",
    "# confidence_level=0.90 means 90% interval (same as old alpha=0.10).\n",
    "y_pred, y_pis = mapie.predict(X_test, confidence_level=0.90)  # 90% interval\n",
    "lo_90 = y_pis[:, 0, 0]\n",
    "hi_90 = y_pis[:, 1, 0]\n",
    "\n",
    "conf_out = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred, \"lo_90\": lo_90, \"hi_90\": hi_90}, index=test_f.index)\n",
    "\n",
    "# empirical coverage\n",
    "coverage = np.mean((conf_out[\"y_true\"] >= conf_out[\"lo_90\"]) & (conf_out[\"y_true\"] <= conf_out[\"hi_90\"]))\n",
    "print(\"Empirical 90% coverage:\", coverage)\n",
    "\n",
    "conf_out.head()\n",
    "\n",
    "# ── Evaluate (central 80%: P10-P90) ──\n",
    "y_pred, y_pis = mapie.predict(X_test, confidence_level=0.80)\n",
    "p10 = y_pis[:, 0, 0]\n",
    "p90 = y_pis[:, 1, 0]\n",
    "p50 = y_pred\n",
    "\n",
    "# Store P10/P90 in conf_out for the rolling coverage comparison\n",
    "conf_out[\"p10\"] = p10\n",
    "conf_out[\"p90\"] = p90\n",
    "\n",
    "plot_forecast_with_interval(\n",
    "    idx=test_f.index,\n",
    "    y_true=y_test.values,\n",
    "    p50=p50, p10=p10, p90=p90,\n",
    "    title=\"Conformal intervals (EnbPI) on top of point model (P10-P90 approx)\"\n",
    ")\n",
    "\n",
    "m_conformal = evaluate_probabilistic_forecast(y_test.values, p10, p50, p90, label=\"Conformal-EnbPI\")\n",
    "print_metrics(m_conformal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a01b1e",
   "metadata": {},
   "source": [
    "### Production notes\n",
    "\n",
    "- This is one of the simplest ways to stop shipping overconfident intervals.\n",
    "- You still need to choose an appropriate conformal method for your temporal dependence regime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b81e5",
   "metadata": {},
   "source": [
    "## 5) Deep probabilistic global model: DeepAR in GluonTS\n",
    "\n",
    "DeepAR is a canonical probabilistic deep model for forecasting and is designed to train across many series. GluonTS focuses on probabilistic forecasting.\n",
    "\n",
    "Below is a minimal example for one series (the real payoff is multiple series). For notebook learning, it's still useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gluonts torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "from gluonts.torch.distributions import StudentTOutput\n",
    "# NOTE: gluonts.torch.trainer.Trainer was removed in GluonTS 0.16+.\n",
    "# Training params now go into trainer_kwargs; batch_size and lr are top-level.\n",
    "\n",
    "# GluonTS expects: start timestamp + target array\n",
    "training_data = ListDataset(\n",
    "    [{\"start\": train.index[0], \"target\": train[\"y\"].values}],\n",
    "    freq=\"D\",\n",
    ")\n",
    "\n",
    "# DeepAR probabilistic model\n",
    "estimator = DeepAREstimator(\n",
    "    freq=\"D\",\n",
    "    prediction_length=h,\n",
    "    context_length=60,\n",
    "    distr_output=StudentTOutput(),  # heavier tails often help\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    trainer_kwargs=dict(max_epochs=10),\n",
    ")\n",
    "\n",
    "predictor = estimator.train(training_data)\n",
    "\n",
    "# Make forecast\n",
    "test_data = ListDataset(\n",
    "    [{\"start\": train.index[0], \"target\": df[\"y\"].values}],\n",
    "    freq=\"D\",\n",
    ")\n",
    "\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "forecast_it, ts_it = make_evaluation_predictions(dataset=test_data, predictor=predictor, num_samples=200)\n",
    "\n",
    "forecast = next(forecast_it)\n",
    "\n",
    "# quantiles\n",
    "p10 = forecast.quantile(0.1)\n",
    "p50 = forecast.quantile(0.5)\n",
    "p90 = forecast.quantile(0.9)\n",
    "\n",
    "deepar_out = pd.DataFrame(\n",
    "    {\"p10\": p10, \"p50\": p50, \"p90\": p90, \"y_true\": test[\"y\"].values},\n",
    "    index=test.index\n",
    ")\n",
    "deepar_out.head()\n",
    "\n",
    "# ── Plot + Evaluate ──\n",
    "p10 = deepar_out[\"p10\"].values\n",
    "p50 = deepar_out[\"p50\"].values\n",
    "p90 = deepar_out[\"p90\"].values\n",
    "\n",
    "plot_forecast_with_interval(\n",
    "    idx=deepar_out.index,\n",
    "    y_true=deepar_out[\"y_true\"].values,\n",
    "    p50=p50, p10=p10, p90=p90,\n",
    "    title=\"DeepAR forecast (P10-P90 from samples)\"\n",
    ")\n",
    "\n",
    "m_deepar = evaluate_probabilistic_forecast(deepar_out[\"y_true\"].values, p10, p50, p90, label=\"DeepAR\")\n",
    "print_metrics(m_deepar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b9a40",
   "metadata": {},
   "source": [
    "**Note:** Adding covariates in GluonTS is very doable (dynamic real features / static categorical features), but the exact dataset schema depends on whether your covariates are known-future vs observed-only; GluonTS docs walk through those patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387890e",
   "metadata": {},
   "source": [
    "## 6) Deep SOTA for covariates: TFT (PyTorch Forecasting)\n",
    "\n",
    "If you want one deep model that is purpose-built for:\n",
    "- multi-horizon outputs\n",
    "- known/unknown covariates\n",
    "- categorical embeddings\n",
    "- quantile forecasts\n",
    "\n",
    "...TFT is the standard \"practical SOTA\" choice, and PyTorch Forecasting is the most common implementation used by practitioners.\n",
    "\n",
    "Here is a minimal runnable example on the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytorch-forecasting pytorch-lightning torch\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "# Build a single-series \"group\"\n",
    "tft_df = df.reset_index().rename(columns={\"ds\":\"time\"})\n",
    "tft_df[\"series\"] = \"S1\"\n",
    "tft_df[\"time_idx\"] = np.arange(len(tft_df))\n",
    "\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = h\n",
    "\n",
    "training_cutoff = tft_df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    tft_df[tft_df.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"y\",\n",
    "    group_ids=[\"series\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    # covariates:\n",
    "    time_varying_known_reals=[\"time_idx\", \"promo\", \"price\"],\n",
    "    time_varying_unknown_reals=[\"y\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"series\"]),\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, tft_df, predict=True, stop_randomization=True)\n",
    "\n",
    "train_loader = training.to_dataloader(train=True, batch_size=64, num_workers=0)\n",
    "val_loader = validation.to_dataloader(train=False, batch_size=64, num_workers=0)\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=3e-3,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    loss=QuantileLoss(quantiles=[0.1, 0.5, 0.9]),\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_loader, val_loader)\n",
    "\n",
    "raw_predictions, x = tft.predict(val_loader, mode=\"raw\", return_x=True)\n",
    "# raw_predictions[\"prediction\"] shape: [batch, horizon, n_quantiles]\n",
    "pred = raw_predictions[\"prediction\"][0].detach().cpu().numpy()  # first batch element\n",
    "\n",
    "p10, p50, p90 = pred[:, 0], pred[:, 1], pred[:, 2]\n",
    "tft_out = pd.DataFrame({\"p10\": p10, \"p50\": p50, \"p90\": p90, \"y_true\": test[\"y\"].values}, index=test.index)\n",
    "tft_out.head()\n",
    "\n",
    "# ── Plot + Evaluate ──\n",
    "p10 = tft_out[\"p10\"].values\n",
    "p50 = tft_out[\"p50\"].values\n",
    "p90 = tft_out[\"p90\"].values\n",
    "\n",
    "plot_forecast_with_interval(\n",
    "    idx=tft_out.index,\n",
    "    y_true=tft_out[\"y_true\"].values,\n",
    "    p50=p50, p10=p10, p90=p90,\n",
    "    title=\"TFT quantile forecast (P10-P90)\"\n",
    ")\n",
    "\n",
    "m_tft = evaluate_probabilistic_forecast(tft_out[\"y_true\"].values, p10, p50, p90, label=\"TFT\")\n",
    "print_metrics(m_tft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191f420",
   "metadata": {},
   "source": [
    "### Production notes\n",
    "\n",
    "- TFT is heavier than GBDT. It shines when you have: many series, lots of covariates, nonlinear interactions, regime changes.\n",
    "- You need disciplined backtesting and careful covariate governance (especially known-future features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ccc3a",
   "metadata": {},
   "source": [
    "## 7) Amazon Chronos-2 (zero-shot) with covariates\n",
    "\n",
    "Chronos-2 is a pretrained foundation model for time series forecasting that works zero-shot (no training needed). It can leverage covariates for improved forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chronos-forecasting\n",
    "import pandas as pd\n",
    "from chronos import Chronos2Pipeline\n",
    "\n",
    "# --- Build Chronos input frames (single series) ---\n",
    "# Chronos expects columns: id, timestamp, target (+ optional covariates)\n",
    "context_df = (\n",
    "    train.reset_index()\n",
    "         .rename(columns={\"ds\": \"timestamp\", \"y\": \"target\"})\n",
    "         .assign(id=\"S1\")[[\"id\", \"timestamp\", \"target\", \"promo\", \"price\"]]\n",
    ")\n",
    "\n",
    "# Future covariates (known-in-advance): include timestamps + covariate columns, no target\n",
    "future_df = (\n",
    "    test.reset_index()\n",
    "        .rename(columns={\"ds\": \"timestamp\"})\n",
    "        .assign(id=\"S1\")[[\"id\", \"timestamp\", \"promo\", \"price\"]]\n",
    ")\n",
    "\n",
    "# --- Load pretrained Chronos-2 ---\n",
    "# device_map=\"cpu\" works everywhere (GPU: \"cuda\")\n",
    "pipeline = Chronos2Pipeline.from_pretrained(\"amazon/chronos-2\", device_map=\"cpu\")\n",
    "\n",
    "# --- Predict quantiles ---\n",
    "pred_df = pipeline.predict_df(\n",
    "    df=context_df,                    # context data with target + covariates\n",
    "    future_df=future_df,              # enables covariate-informed forecasting\n",
    "    prediction_length=h,\n",
    "    quantile_levels=[0.1, 0.5, 0.9],\n",
    "    id_column=\"id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    target=\"target\",\n",
    ")\n",
    "\n",
    "# pred_df typically contains: timestamp, predictions (central), and quantile columns \"0.1\",\"0.5\",\"0.9\"\n",
    "pred_df = pred_df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "p10 = pred_df[\"0.1\"].values\n",
    "p50 = pred_df[\"0.5\"].values if \"0.5\" in pred_df.columns else pred_df[\"predictions\"].values\n",
    "p90 = pred_df[\"0.9\"].values\n",
    "y_true = test[\"y\"].values\n",
    "\n",
    "chronos_out = pd.DataFrame({\"p10\": p10, \"p50\": p50, \"p90\": p90, \"y_true\": y_true}, index=test.index)\n",
    "chronos_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot ---\n",
    "plot_forecast_with_interval(\n",
    "    idx=test.index,\n",
    "    y_true=y_true,\n",
    "    p50=p50, p10=p10, p90=p90,\n",
    "    title=\"Chronos-2 (zero-shot) forecast (P10–P90)\"\n",
    ")\n",
    "\n",
    "# --- Evaluate ---\n",
    "m_chronos = evaluate_probabilistic_forecast(y_true, p10, p50, p90, label=\"Chronos-2\")\n",
    "print_metrics(m_chronos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33abbe",
   "metadata": {},
   "source": [
    "### Production notes\n",
    "\n",
    "- Chronos-2 is zero-shot (no training needed), making it extremely fast to deploy for new series.\n",
    "- Works well out-of-the-box but may underperform task-specific fine-tuned models on domains with strong patterns.\n",
    "- Ideal for cold-start scenarios, exploratory analysis, or as a strong baseline before investing in custom models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db50d0",
   "metadata": {},
   "source": [
    "## Calibration Section: Compare All Methods\n",
    "\n",
    "Once you've run all models, this section compares them side-by-side and provides calibration diagnostics.\n",
    "\n",
    "**Note:** Only SARIMAX, LightGBM, and NGBoost include rolling-origin backtests above (refitting deep models per fold is expensive in a notebook). The comparison table below reflects single-holdout performance for all 7 methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = [m_sarimax, m_lgbm, m_ngboost, m_conformal, m_deepar, m_tft, m_chronos]\n",
    "metrics_df = pd.DataFrame(all_metrics).set_index(\"model\").sort_values(\"WIS_proxy(10/50/90)\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b2998",
   "metadata": {},
   "source": [
    "### Rolling Coverage Curve\n",
    "\n",
    "A rolling coverage curve shows how coverage varies over time, which is more informative than a single coverage number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_coverage_curve(y_true, lo, hi, window=14):\n",
    "    y = np.asarray(y_true)\n",
    "    inside = ((y >= np.asarray(lo)) & (y <= np.asarray(hi))).astype(float)\n",
    "    return pd.Series(inside).rolling(window).mean().values\n",
    "\n",
    "# Collect P10/P90 for all methods\n",
    "ci_80_sarimax = res.get_forecast(steps=h, exog=test[exog_cols]).conf_int(alpha=0.20)\n",
    "\n",
    "coverage_data = {\n",
    "    \"SARIMAX\":    (ci_80_sarimax.iloc[:, 0].values, ci_80_sarimax.iloc[:, 1].values),\n",
    "    \"LightGBM\":   (lgb_out[\"p10\"].values, lgb_out[\"p90\"].values),\n",
    "    \"NGBoost\":    (ngb_out[\"p10\"].values, ngb_out[\"p90\"].values),\n",
    "    \"Conformal\":  (conf_out[\"p10\"].values, conf_out[\"p90\"].values),\n",
    "    \"DeepAR\":     (deepar_out[\"p10\"].values, deepar_out[\"p90\"].values),\n",
    "    \"TFT\":        (tft_out[\"p10\"].values, tft_out[\"p90\"].values),\n",
    "    \"Chronos-2\":  (chronos_out[\"p10\"].values, chronos_out[\"p90\"].values),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for label, (lo, hi) in coverage_data.items():\n",
    "    cov_curve = rolling_coverage_curve(test[\"y\"].values, lo, hi, window=14)\n",
    "    plt.plot(test.index, cov_curve, label=label)\n",
    "\n",
    "plt.axhline(0.8, linestyle=\"--\", color=\"black\", alpha=0.5, label=\"nominal 0.8\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.ylabel(\"Rolling 14-day coverage\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.title(\"Rolling interval coverage (P10-P90) - all methods\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tsprob)",
   "language": "python",
   "name": "tsprob"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
